{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be28f7d",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486e21fa",
   "metadata": {},
   "source": [
    "**Part 1: Data Cleaning, EDA and Feature Engineering** \n",
    "- Train Data Cleaning, EDA and Feature Engineering\n",
    "- Weather Data Cleaning, EDA and Feature Engineering\n",
    "- Spray Data Cleaning, EDA and Feature Engineering\n",
    "- Combined EDA and Feature Engineering\n",
    "\n",
    "**Part 2: Preprocessing, Feature Selection and Baseline Model** \n",
    "- Pre-processing\n",
    "- Feature Selection\n",
    "- Baseline Model\n",
    "\n",
    "**Part 3: Modelling**\n",
    "- Model Fit and Testing\n",
    "- Model Iteration\n",
    "- Model Evaluation\n",
    "- Conclusion & Recommending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47036dc",
   "metadata": {},
   "source": [
    "## Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c46c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np # Version 1.20.1\n",
    "import pandas as pd # Version 1.2.4\n",
    "# Pandas settings\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Plotting imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('ticks')\n",
    "# Please reduce \"DPI\" values below if plots take too long to display \n",
    "# Higher values make plots remain clear when zoomed in\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "# The lowest recommended value is 200 for A4 size print legibility \n",
    "# The recommended value is 600 for 'photograph-like' legibility\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Time Series imports\n",
    "import datetime as dt # Version 2.8.1\n",
    "\n",
    "# Pre-Processing imports\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as pipe\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "# Modelling imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, plot_roc_curve, precision_recall_fscore_support,  roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d38df",
   "metadata": {},
   "source": [
    "## Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7343d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ac5e072",
   "metadata": {},
   "source": [
    "## Pre-precessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ee6b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = pd.read_csv('../data/train_combined.csv')\n",
    "test = pd.read_csv('../data/test.csv', parse_dates = ['Date'])\n",
    "weather_combined = pd.read_csv('../data/weather_combined.csv', parse_dates = ['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "129396cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined.drop(columns = 'Unnamed: 0', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decf385b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116293, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b67c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged = test.merge(weather_combined[weather_combined['Station']==1], on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88897bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'Date', 'Address', 'Species', 'Block', 'Street', 'Trap',\n",
       "       'AddressNumberAndStreet', 'Latitude', 'Longitude', 'AddressAccuracy',\n",
       "       'Unnamed: 0', 'index', 'Station', 'Tmax', 'Tmin', 'Tavg', 'DewPoint',\n",
       "       'WetBulb', 'Heat', 'Cool', 'Sunrise', 'Sunset', 'Wet_NoWet',\n",
       "       'PrecipTotal', 'StnPressure', 'SeaLevel', 'ResultSpeed', 'ResultDir',\n",
       "       'AvgSpeed', 'Year', 'Month', 'WeekofYear', 'Daylight_Hours', 'Tmin_rm7',\n",
       "       'DewPoint_rm7', 'Sunrise_rm7', 'PrecipTotal_rm7', 'StnPressure_rm7',\n",
       "       'ResultSpeed_rm7', 'ResultDir_rm7', 'Tmin_rm7_lag1', 'Tmin_rm7_lag2',\n",
       "       'Tmin_rm7_lag3', 'Tmin_rm7_lag4', 'DewPoint_rm7_lag1',\n",
       "       'DewPoint_rm7_lag2', 'DewPoint_rm7_lag3', 'DewPoint_rm7_lag4',\n",
       "       'Sunrise_rm7_lag1', 'Sunrise_rm7_lag2', 'Sunrise_rm7_lag3',\n",
       "       'Sunrise_rm7_lag4', 'PrecipTotal_rm7_lag1', 'PrecipTotal_rm7_lag2',\n",
       "       'PrecipTotal_rm7_lag3', 'PrecipTotal_rm7_lag4', 'StnPressure_rm7_lag1',\n",
       "       'StnPressure_rm7_lag2', 'StnPressure_rm7_lag3', 'StnPressure_rm7_lag4',\n",
       "       'ResultSpeed_rm7_lag1', 'ResultSpeed_rm7_lag2', 'ResultSpeed_rm7_lag3',\n",
       "       'ResultSpeed_rm7_lag4', 'ResultDir_rm7_lag1', 'ResultDir_rm7_lag2',\n",
       "       'ResultDir_rm7_lag3', 'ResultDir_rm7_lag4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1873a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_merged.drop(columns = ['Id','Address','Block','Street','AddressNumberAndStreet','AddressAccuracy','Unnamed: 0'], inplace = True)\n",
    "# Dropping identified collinear weather features\n",
    "test_merged.drop(columns = ['Tmax','Tavg','Heat','Cool'], inplace = True)\n",
    "test_merged.drop(columns = 'WetBulb', inplace = True)\n",
    "test_merged.drop(columns = ['Sunset','Daylight_Hours'], inplace = True)\n",
    "test_merged.drop(columns = ['SeaLevel','AvgSpeed'], inplace = True)\n",
    "\n",
    "# Drop Trap column as traps are identifiable through Latitude and Longitude values\n",
    "test_merged.drop(columns = 'Trap', inplace = True)\n",
    "# Drop the Station there is only one unique value(Station 1)\n",
    "test_merged.drop(columns = 'Station', inplace = True)\n",
    "# Drop roll mean columns as train dataset interval is weekly\n",
    "test_merged.drop(columns = ['Tmin_rm7','DewPoint_rm7','Sunrise_rm7','PrecipTotal_rm7','StnPressure_rm7','ResultSpeed_rm7','ResultDir_rm7'], inplace = True)\n",
    "# Drop index column from merge\n",
    "test_merged.drop(columns = 'index', inplace = True)\n",
    "\n",
    "# Create new date & time related columns; i.e. month and weekofyear\n",
    "test_merged['Month'] = test_merged['Date'].dt.month\n",
    "test_merged['WeekofYear'] = test_merged['Date'].dt.isocalendar().week\n",
    "# Drop date as we no longer need with the new dt columns\n",
    "test_merged.drop(columns = 'Date', inplace = True)\n",
    "# Encode Species column to indicate 1 for species that carry WNV and 0 for species that do not \n",
    "test_merged['Species'] = test_merged['Species'].apply(lambda x: 1 if x in ['CULEX PIPIENS','CULEX PIPIENS/RESTUANS','CULEX RESTUANS'] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc4df05a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8610, 43)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db30b3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116293, 42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3782343e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2008, 2010, 2012, 2014], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_merged['Year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa633f",
   "metadata": {},
   "source": [
    "### Spliting train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df561d4",
   "metadata": {},
   "source": [
    "Here is an outline of how the train test datasets will be transformed:\n",
    "- Train data to be split into four sets consisting of the following years:\n",
    "    - train1 (2007)\n",
    "    - train2 (2007,2009)\n",
    "    - train3 (2007,2009,2011)\n",
    "    - train4 (2009,2011,2013)  \n",
    "      \n",
    "      \n",
    "- Test data to be split into four sets consisting of the following years:\n",
    "    - test1 (2008)\n",
    "    - test2 (2010)\n",
    "    - test3 (2012)\n",
    "    - test4 (2014)  \n",
    "    \n",
    "In practice, we very likely will retrain our model as new data becomes available.\n",
    "\n",
    "This would give the model the best opportunity to make good forecasts at each time step. We can evaluate our machine learning models under this assumption.\n",
    "\n",
    "Starting at the beginning of the time series, the minimum number of samples in the window is used to train a model.\n",
    "1. The model makes a prediction for the next time step.\n",
    "2. The prediction is stored or evaluated against the known value.\n",
    "3. The window is expanded to include the known value and the process is repeated (go to step 1.)  \n",
    "\n",
    "Because this methodology involves moving along the time series one-time step at a time, it is often called Walk Forward Testing or Walk Forward Validation. Additionally, because a sliding or expanding window is used to train a model, this method is also referred to as Rolling Window Analysis or a Rolling Forecast.  \n",
    "\n",
    "It should also be important to note the above methodology cannot be applied to this project as ideally once a year has passed in the present we would have obtained the ground truth for that year for both comparing model performance and updating the train sets to include years passed (we did not include test years passed as the training data for subsequent years).\n",
    "\n",
    "The rationale for the split is in pursuit of realism, for time-series predictions there are assumptions made where past data are correlated to the present. Not only is past data correlated, another assumption is that the most recent past data have a natural tendency to be the most correlated.\n",
    "\n",
    "Secondly, it would not be realistically correct to have future data(which is what we are trying to predict) available to train models, which would be the case if we were to use models trained on the entire train dataset (every alt. year from 2007 - 2013) to predict for year 2008.\n",
    "\n",
    "Thirdly, we decided to limit the maximum number of years in a training set to 3 years, this will allow us to keep only the three most recent years and also prevent an ever expanding training set which will become more computationally intensive in both resource and time to maintain and train.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting of train dataset into the outlined years\n",
    "tr01 = train_combined[train_combined['Year'] == 2007]\n",
    "tr02 = train_combined[(train_combined['Year'] == 2007) | (train_combined['Year'] == 2009)]\n",
    "tr03 = train_combined[(train_combined['Year'] == 2007) | (train_combined['Year'] == 2009) | (train_combined['Year'] == 2011)]\n",
    "tr04 = train_combined[(train_combined['Year'] == 2009) | (train_combined['Year'] == 2011) | (train_combined['Year'] == 2013)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "916fd697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting of test dataset into the outlined years\n",
    "tst01 = test_merged[test_merged['Year'] == 2008]\n",
    "tst02 = test_merged[test_merged['Year'] == 2010]\n",
    "tst03 = test_merged[test_merged['Year'] == 2012]\n",
    "tst04 = test_merged[test_merged['Year'] == 2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fdd6f",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1c8dd",
   "metadata": {},
   "source": [
    "As we split our train dataset into 4, the baseline models for each respective sets will be their respective normalized value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19ec0ae1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset1\n",
      "(2837, 43)\n",
      "0    0.93338\n",
      "1    0.06662\n",
      "Name: WnvPresent, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"trainset1\")\n",
    "print(tr01.shape)\n",
    "print(tr01['WnvPresent'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f587246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset2\n",
      "(4758, 43)\n",
      "0    0.956284\n",
      "1    0.043716\n",
      "Name: WnvPresent, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"trainset2\")\n",
    "print(tr02.shape)\n",
    "print(tr02['WnvPresent'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c3df8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset3\n",
      "(6552, 43)\n",
      "0    0.960623\n",
      "1    0.039377\n",
      "Name: WnvPresent, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"trainset3\")\n",
    "print(tr03.shape)\n",
    "print(tr03['WnvPresent'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9954e6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset4\n",
      "(5773, 43)\n",
      "0    0.953577\n",
      "1    0.046423\n",
      "Name: WnvPresent, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"trainset4\")\n",
    "print(tr04.shape)\n",
    "print(tr04['WnvPresent'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac70d81b",
   "metadata": {},
   "source": [
    "While we have fluctuating percentage values for our negative class (WnvPresent:0), imbalanced classes is common across all training sets, we will deal with it using `SMOTE`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30eef0",
   "metadata": {},
   "source": [
    "## Metrics for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596bedf0",
   "metadata": {},
   "source": [
    "While the `AUC-ROC` score will be evaluated for Kaggle, the more important metric to focus on for the problem statement is false negatives. \n",
    "\n",
    "False negatives happen when the model wrongly predicts no presence of WNV when it is actually present. It will be important to ensure minimal false negatives these may translate to outbreaks causing enormous strain on public healthcare and ineffective allocation of resources. The model metric allowing us to measure model performance with regards to minimizing false negatives is `Recall` score.\n",
    "\n",
    "Ultimately, when evaluating model performance we will prioritize choosing models with primarily high recall score, followed by AUC-ROC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b7a9029",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "smt = SMOTE(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06debb3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fad7e1e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 2837\n",
      "Training Observations: 2269\n",
      "Validation Observations: 568\n",
      "X_tr01 scaled sampled  observations: 4168\n",
      "y_tr01 sampled observations: 4168\n"
     ]
    }
   ],
   "source": [
    "# Define X and Y for tr01\n",
    "X = tr01.drop(columns=['WnvPresent'])\n",
    "y = tr01['WnvPresent']\n",
    "\n",
    "# Train,test split using date based slicing\n",
    "train_size = int(len(X) * 0.80)\n",
    "X_tr01, X_val01 = X[0:train_size], X[train_size:len(X)]\n",
    "y_tr01, y_val01 = y[0:train_size], y[train_size:len(X)]\n",
    "print('Observations: %d' % (len(X)))\n",
    "print('Training Observations: %d' % (len(X_tr01)))\n",
    "print('Validation Observations: %d' % (len(X_val01)))\n",
    "\n",
    "# Scaling X_train , X_val\n",
    "X_tr01sc = ss.fit_transform(X_tr01)\n",
    "X_val01sc = ss.transform(X_val01)\n",
    "\n",
    "# Oversampling using SMOTE\n",
    "X_tr01sc_smt, y_tr01_smt = smt.fit_resample(X_tr01sc, y_tr01)\n",
    "print('X_tr01 scaled sampled  observations: %d' % (len(X_tr01sc_smt)))\n",
    "print('y_tr01 sampled observations: %d' % (len(y_tr01_smt)))\n",
    "\n",
    "# trainset1 modelling final variables\n",
    "# X_train, y_train : X_tr01sc_smt, y_tr01_smt\n",
    "# X_val, y_val : X_val01sc, y_val01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d994dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 4758\n",
      "Training Observations: 3806\n",
      "Validation Observations: 952\n",
      "X_tr02 scaled sampled  observations: 7230\n",
      "y_tr02 sampled observations: 7230\n"
     ]
    }
   ],
   "source": [
    "# Define X and Y for tr02\n",
    "X = tr02.drop(columns=['WnvPresent'])\n",
    "y = tr02['WnvPresent']\n",
    "\n",
    "# Train,test split using date based slicing\n",
    "train_size = int(len(X) * 0.80)\n",
    "X_tr02, X_val02 = X[0:train_size], X[train_size:len(X)]\n",
    "y_tr02, y_val02 = y[0:train_size], y[train_size:len(X)]\n",
    "print('Observations: %d' % (len(X)))\n",
    "print('Training Observations: %d' % (len(X_tr02)))\n",
    "print('Validation Observations: %d' % (len(X_val02)))\n",
    "\n",
    "# Scaling X_train , X_val\n",
    "X_tr02sc = ss.fit_transform(X_tr02)\n",
    "X_val02sc = ss.transform(X_val02)\n",
    "\n",
    "# Oversampling using SMOTE\n",
    "X_tr02sc_smt, y_tr02_smt = smt.fit_resample(X_tr02sc, y_tr02)\n",
    "print('X_tr02 scaled sampled  observations: %d' % (len(X_tr02sc_smt)))\n",
    "print('y_tr02 sampled observations: %d' % (len(y_tr02_smt)))\n",
    "\n",
    "# trainset1 modelling final variables\n",
    "# X_train, y_train : X_tr02sc_smt, y_tr02_smt\n",
    "# X_val, y_val : X_val02sc, y_val02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4973d9f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 6552\n",
      "Training Observations: 5241\n",
      "Validation Observations: 1311\n",
      "X_tr03 scaled sampled  observations: 10066\n",
      "y_tr03 sampled observations: 10066\n"
     ]
    }
   ],
   "source": [
    "# Define X and Y for tr03\n",
    "X = tr03.drop(columns=['WnvPresent'])\n",
    "y = tr03['WnvPresent']\n",
    "\n",
    "# Train,test split using date based slicing\n",
    "train_size = int(len(X) * 0.80)\n",
    "X_tr03, X_val03 = X[0:train_size], X[train_size:len(X)]\n",
    "y_tr03, y_val03 = y[0:train_size], y[train_size:len(X)]\n",
    "print('Observations: %d' % (len(X)))\n",
    "print('Training Observations: %d' % (len(X_tr03)))\n",
    "print('Validation Observations: %d' % (len(X_val03)))\n",
    "\n",
    "# Scaling X_train , X_val\n",
    "X_tr03sc = ss.fit_transform(X_tr03)\n",
    "X_val03sc = ss.transform(X_val03)\n",
    "\n",
    "# Oversampling using SMOTE\n",
    "X_tr03sc_smt, y_tr03_smt = smt.fit_resample(X_tr03sc, y_tr03)\n",
    "print('X_tr03 scaled sampled  observations: %d' % (len(X_tr03sc_smt)))\n",
    "print('y_tr03 sampled observations: %d' % (len(y_tr03_smt)))\n",
    "\n",
    "# trainset1 modelling final variables\n",
    "# X_train, y_train : X_tr03sc_smt, y_tr03_smt\n",
    "# X_val, y_val : X_val03sc, y_val03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a552e6cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 5773\n",
      "Training Observations: 4618\n",
      "Validation Observations: 1155\n",
      "X_tr04 scaled sampled  observations: 9046\n",
      "y_tr04 sampled observations: 9046\n"
     ]
    }
   ],
   "source": [
    "# Define X and Y for tr04\n",
    "X = tr04.drop(columns=['WnvPresent'])\n",
    "y = tr04['WnvPresent']\n",
    "\n",
    "# Train,test split using date based slicing\n",
    "train_size = int(len(X) * 0.80)\n",
    "X_tr04, X_val04 = X[0:train_size], X[train_size:len(X)]\n",
    "y_tr04, y_val04 = y[0:train_size], y[train_size:len(X)]\n",
    "print('Observations: %d' % (len(X)))\n",
    "print('Training Observations: %d' % (len(X_tr04)))\n",
    "print('Validation Observations: %d' % (len(X_val04)))\n",
    "\n",
    "# Scaling X_train , X_val\n",
    "X_tr04sc = ss.fit_transform(X_tr04)\n",
    "X_val04sc = ss.transform(X_val04)\n",
    "\n",
    "# Oversampling using SMOTE\n",
    "X_tr04sc_smt, y_tr04_smt = smt.fit_resample(X_tr04sc, y_tr04)\n",
    "print('X_tr04 scaled sampled  observations: %d' % (len(X_tr04sc_smt)))\n",
    "print('y_tr04 sampled observations: %d' % (len(y_tr04_smt)))\n",
    "\n",
    "# trainset1 modelling final variables\n",
    "# X_train, y_train : X_tr04sc_smt, y_tr04_smt\n",
    "# X_val, y_val : X_val04sc, y_val04"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
